---
author: "Kangx"
title: "How does LSTM avoid gradient explosion or vanish?"
date: "2024-06-11"
tags: ["LSTM","RNN","gradient vanish or explosion"]
ShowToc: true
UseHugoToc: true
TocOpen: false
ShowBreadCrumbs: false
---
{{< figure src="../RNN.png" attr="Fig. 1. RNN architecture" align=center target="_blank" >}}
## 1. Why can't RNN capture long-term dependencies?

$$
h^{(t)} = \tanh(Ux^{(t)}+Wh^{(t-1)}+b)\tag{1}
$$
$$
o^{(t)} = Vh^{t}+c\tag{2}
$$
$$
\hat{y}^{(t)} = \text{softmax}(o^{(t)})\tag{3}
$$

### 1.1. Analyze

We can explain why RNN can't capture long-term dependency from two angles:

- <p>From the angle of \( \frac{\partial \mathrm{h}^{(\mathrm{t})}}{\partial \mathrm{h}^{(1)}} \)
  $$
  \frac{\partial \mathrm{h}^{(\mathrm{t})}}{\partial \mathrm{h}^{(1)}} = 
  \prod_{\mathrm{j}=2}^{\mathrm{t}}\frac{\partial \mathrm{h}^{(\mathrm{j})}}{\partial \mathrm{h}^{(\mathrm{j}-1)}} = \prod_{\mathrm{j}=2}^{\mathrm{t}}\tanh'\cdot W\tag{4}
  $$
  The derivative \( \frac{\partial \mathrm{h}^{(\mathrm{t})}}{\partial \mathrm{h}^{(1)}} \) is essentially telling us how much our hidden state at time \(t\) will change when we change the hidden state at time \(1\) by a little bit. According to the above math, if the gradient vanishes it means the earlier hidden states have no real effect on the later hidden states, meaning no long term dependencies are learned!
  </p>

- <p>From the angle of \(\frac{\partial L}{\partial r_t}\)
    <blockquote>
    <i>See ML book of Li Hang P453</i>
    </blockquote>
    
  $$
  \frac{\partial L}{\partial r_t} = \text{diag}(1-\tanh^2r_t)\cdot U^T\cdot \frac{\partial L}{\partial r_{t+1}}+\text{diag}(1-\tanh^2r_t)\cdot V^T\cdot \frac{\partial L}{\partial z_t}\tag{5}
  $$
  $$
  r_t = U\cdot h_{t-1}+W\cdot x_t+b\tag{6}
  $$
  </p>
  
  <p>
  From the above formulas, we can observe that when \(t\) is very small and the dominant eigenvalue of the matrix \(U\) is less than 1, \(\frac{\partial L}{\partial r_t}\) will be close to 0. As such, if we change \(r_t\) by a little bit, loss \(L\) will basically not change. If our task relies more on historical information, this is obviously unreasonable.
  </p>
## 2. LSTM
### 2.1. Forward
$$
f_t=\sigma\left(W_f\left[h_{t-1}, x_t\right]\right)\tag{6}
$$

$$
    i_t=\sigma\left(W_i\left[h_{t-1}, x_t\right]\right)\tag{7}
$$

$$
o_t=\sigma\left(W_o\left[h_{t-1}, x_t\right]\right)\tag{8}
$$

<p>
$$
\widetilde{C}_t=\tanh \left( W_C \left[ h_{t-1}, x_t \right] \right) \tag{9}
$$
</p>

$$
C_t=f_t C_{t-1}+i_t \widetilde{C}_t\tag{10}
$$

$$
h_t=o_t \tanh \left(C_t\right)\tag{11}
$$
<p>
We can calculate \(\frac{\partial \mathrm{h}^{(\mathrm{j})}}{\partial \mathrm{h}^{(\mathrm{j}-1)}}\) as above, but from Eqn.(11) we can see that analyzing \(C_t\) is equivalent to analyzing \(h_t\), and calculating \(\frac{\partial \mathrm{C}^{(\mathrm{t})}}{\partial \mathrm{C}^{(\mathrm{t}-1)}}\) is simpler.
</p>
<p>
$$
\frac{\partial C_t}{\partial C_{t-1}}  =C_{t-1} \sigma^{\prime}(\cdot) W_f * o_{t-1} \tanh ^{\prime}\left(C_{t-1}\right) \newline
 +\widetilde{C}_t \sigma^{\prime}(\cdot) W_i * o_{t-1} \tanh ^{\prime}\left(C_{t-1}\right) \newline
 +i_t \tanh ^{\prime}(\cdot) W_C * o_{t-1} \tanh ^{\prime}\left(C_{t-1}\right) \newline
 +f_t\tag{12}
$$
</p>

<p>
Now if we want to backpropagate back \(k\) time steps, we simply multiply terms in the form of the one above 
\(k\) times.The terms here, \(\frac{\partial C_t}{\partial C_{t-1}}\), at any time step can take on either values that are greater than 1 or values in the range 
[0,1].
</p>
<p>
If we start to converge to zero, we can always set the values of \(f_t\) (and other gate values) to be higher in order to bring the value of \(\frac{\partial C_t}{\partial C_{t-1}}\) closer to 1, thus preventing the gradients from vanishing (or at the very least, preventing them from vanishing too quickly). One important thing to note is that the values 
\(f_t, o_t, i_t\), and \(\widetilde{C}_t\)
 are things that the network learns to set (conditioned on the current input and hidden state). Thus, in this way the network learns to decide when to let the gradient vanish, and when to preserve it, by setting the gate values accordingly!
</p>

## Reference
[1] [Why LSTMs Stop Your Gradients From Vanishing: A View from the Backwards Pass](https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html) 

[2] [Let's also talk about the vanishing/exploding gradient problem of RNN By Su Jianlin](https://kexue.fm/archives/7888)

[3] [Why can LSTM alleviate vanishing gradients?](https://blog.csdn.net/zhaojc1995/article/details/114649486)
